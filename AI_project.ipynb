{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install memory-profiler\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC2Z23GCWl9t",
        "outputId": "ee85311c-76f1-49f7-bef4-a1b2fe23a24a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting memory-profiler\n",
            "  Downloading memory_profiler-0.61.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from memory-profiler) (5.9.5)\n",
            "Downloading memory_profiler-0.61.0-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: memory-profiler\n",
            "Successfully installed memory-profiler-0.61.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V5qwZN4-WFXl"
      },
      "outputs": [],
      "source": [
        "import heapq\n",
        "from collections import deque\n",
        "from tkinter import Tk, Canvas\n",
        "import time\n",
        "from memory_profiler import profile\n",
        "import random\n",
        "import math\n",
        "from itertools import count\n",
        "\n",
        "class Pair:\n",
        "    def _init_(self, first, second, parentF, parentS, depth, costFromStart=0):\n",
        "        self.first = first\n",
        "        self.second = second\n",
        "        self.parentF = parentF\n",
        "        self.parentS = parentS\n",
        "        self.depth = depth\n",
        "        self.costFromStart = costFromStart\n",
        "\n",
        "    def _lt_(self, other):\n",
        "        return self.costFromStart < other.costFromStart\n",
        "\n",
        "class Maze:\n",
        "  #0 represents open cells and -1 represents walls.\n",
        "    def _init_(self, maze, startX, startY, endX, endY):\n",
        "        self.maze = maze\n",
        "        self.startX = startX\n",
        "        self.startY = startY\n",
        "        self.endX = endX\n",
        "        self.endY = endY\n",
        "        self.visited = [[False for _ in row] for row in maze]\n",
        "        self.Xlength = len(maze)\n",
        "        self.Ylength = len(maze[0])\n",
        "\n",
        "    def visualize_maze(maze):\n",
        "      cell_size = 100  # Size of each cell in the maze\n",
        "      window_width = maze.Ylength * cell_size\n",
        "      window_height = maze.Xlength * cell_size\n",
        "\n",
        "      # Create the tkinter window\n",
        "      window = Tk()\n",
        "      window.title(\"Maze Visualization\")\n",
        "      canvas = Canvas(window, width=window_width, height=window_height, bg=\"white\")\n",
        "      canvas.pack()\n",
        "\n",
        "      # Draw the maze\n",
        "      for row in range(maze.Xlength):\n",
        "          for col in range(maze.Ylength):\n",
        "              x1, y1 = col * cell_size, row * cell_size\n",
        "              x2, y2 = x1 + cell_size, y1 + cell_size\n",
        "              if maze.maze[row][col] == -1:  # Wall\n",
        "                  color = \"black\"\n",
        "              elif (row, col) == (maze.startX, maze.startY):  # Start\n",
        "                  color = \"green\"\n",
        "              elif (row, col) == (maze.endX, maze.endY):  # Goal\n",
        "                  color = \"red\"\n",
        "              else:  # Free cell\n",
        "                  color = \"white\"\n",
        "              canvas.create_rectangle(x1, y1, x2, y2, fill=color, outline=\"gray\")\n",
        "\n",
        "      window.mainloop()\n",
        "\n",
        "\n",
        "\n",
        "## prints the path in the terminal and return it\n",
        "def print_solution(maze, solution, curr):\n",
        "    path=[]\n",
        "    if curr.parentF == maze.startX and curr.parentS == maze.startY:\n",
        "        print(f\"Start: ({curr.parentF}, {curr.parentS}) moved to ({curr.first}, {curr.second}) Depth: {curr.depth}\")\n",
        "        path.append((curr.first, curr.second))\n",
        "        return path\n",
        "    else:\n",
        "        print(f\"Next: ({curr.parentF}, {curr.parentS}) moved to ({curr.first}, {curr.second}) Depth: {curr.depth}\")\n",
        "        path.append((curr.first, curr.second))\n",
        "\n",
        "        for pair in solution:\n",
        "            if pair.first == curr.parentF and pair.second == curr.parentS:\n",
        "                return print_solution(maze, solution, pair)\n",
        "    return path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing the necessary libraries\n"
      ],
      "metadata": {
        "id": "Msm7n6JTaMDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random"
      ],
      "metadata": {
        "id": "QyNH1WKbaPJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. State Space Representation\n"
      ],
      "metadata": {
        "id": "gDsM4VS5aSJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# here we are defining the state space and identifing the initial and goal states by Creates a list of all possible states in the grid and each state is represented as a tuple (x, y)\n"
      ],
      "metadata": {
        "id": "0q3wiHSbaV6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_space = [(x, y) for x in range(5) for y in range(5)]\n"
      ],
      "metadata": {
        "id": "0x9elqPca_mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "initial_state: The starting point of the agent (top-left corner of the grid)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "goal_state: The target the agent is trying to reach (bottom-right corner)\n"
      ],
      "metadata": {
        "id": "FJ-pZTk_byh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_state = (0, 0)\n",
        "goal_state = (4, 4)\n"
      ],
      "metadata": {
        "id": "X_kvBF4db5Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Action Set\n"
      ],
      "metadata": {
        "id": "nDJNLjbzb8zB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "actions = ['up', 'down', 'left', 'right']\n"
      ],
      "metadata": {
        "id": "ow9oEPPlcBDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#calculates the next state based on the current state and the chosen action to nsures the agent doesnâ€™t move outside the boundaries of the grid (0 <= x, y <= 99)."
      ],
      "metadata": {
        "id": "VjCQUGAmcVo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_next_state(state, action):\n",
        "    x, y = state\n",
        "    if action == 'up':\n",
        "        return (max(x - 1, 0), y)  # prevents moving out of the grid\n",
        "    elif action == 'down':\n",
        "        return (min(x + 1, 4), y)  # prevents moving out of the grid\n",
        "    elif action == 'left':\n",
        "        return (x, max(y - 1, 0))  # prevents moving out of the grid\n",
        "    elif action == 'right':\n",
        "        return (x, min(y + 1, 4))  # prevents moving out of the grid\n",
        "    return state\n"
      ],
      "metadata": {
        "id": "s2nDPJkmcDxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Reward Function\n"
      ],
      "metadata": {
        "id": "qlmovBLocHOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reward_function(state):\n",
        "    if state == goal_state:\n",
        "        return 100\n",
        "    else:\n",
        "        return -1"
      ],
      "metadata": {
        "id": "cvvyHEXLcMf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Q-Learning Setup\n"
      ],
      "metadata": {
        "id": "8IKRj0yOcu8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# here we are createing a Q-table with dimensions which is [number of states, number of actions] in the begging all Q-values are set to 0 (cuz agent has no knowledge)."
      ],
      "metadata": {
        "id": "7-LePyCfczkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q_table = np.zeros((len(state_space), len(actions)))\n"
      ],
      "metadata": {
        "id": "IPyPqCWXcPri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "discount_factor = 0.9\n",
        "epsilon = 1.0  # exploration rate\n",
        "epsilon_decay = 0.995\n",
        "min_epsilon = 0.1"
      ],
      "metadata": {
        "id": "C5YhCVUGdHpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# maps each state to a unique index for easy access in the Q-table"
      ],
      "metadata": {
        "id": "i5n9C2U-dQhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "state_to_index = {state: i for i, state in enumerate(state_space)}\n"
      ],
      "metadata": {
        "id": "f0kT3ypvdOsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train agent function"
      ],
      "metadata": {
        "id": "6RGzG2HZdVhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This function trains the agent using the Q-learning algorithm that we used. The agent starts at the initial state and learns over a specified number of episodes. In each episode, the agent explores or exploits its knowledge by either choosing a random action (exploration) or selecting the best-known action based on the Q-table (exploitation). After performing an action, it moves to the next state, receives a reward based on the environment, and updates the Q-value for the action it just took. If the agent reaches the goal, the episode ends early. The exploration rate (`epsilon`) is reduced gradually after each episode to encourage the agent to exploit its learned policy more as training progresses. This process helps the agent learn the optimal actions to take in different states to maximize its rewards."
      ],
      "metadata": {
        "id": "QMy4jtOadfAO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_agent(episodes=1000, max_steps=100):\n",
        "    global epsilon\n",
        "    for episode in range(episodes):\n",
        "        state = initial_state\n",
        "        for step in range(max_steps):\n",
        "            state_index = state_to_index[state]\n",
        "\n",
        "\n",
        "            if random.uniform(0, 1) < epsilon:\n",
        "                action_index = random.choice(range(len(actions)))\n",
        "            else:\n",
        "                action_index = np.argmax(q_table[state_index])\n",
        "\n",
        "            action = actions[action_index]\n",
        "            next_state = get_next_state(state, action)\n",
        "            next_state_index = state_to_index[next_state]\n",
        "\n",
        "            reward = reward_function(next_state)\n",
        "            q_table[state_index, action_index] = q_table[state_index, action_index] + learning_rate * (\n",
        "                reward + discount_factor * np.max(q_table[next_state_index]) - q_table[state_index, action_index]\n",
        "            )\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "            if state == goal_state:\n",
        "                break\n",
        "\n",
        "        epsilon = max(min_epsilon, epsilon * epsilon_decay)"
      ],
      "metadata": {
        "id": "80M-Qo2kdbEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_agent()"
      ],
      "metadata": {
        "id": "IGrStB9AeYes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Performance Monitoring\n"
      ],
      "metadata": {
        "id": "rC401fMTee6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# printing the Q-values to evaluate learning progress\n"
      ],
      "metadata": {
        "id": "JgV00iw_ehmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_q_values():\n",
        "    print(\"Q-values for each state:\")\n",
        "    for state, index in state_to_index.items():\n",
        "        print(f\"State {state}: {q_table[index]}\")"
      ],
      "metadata": {
        "id": "d2jLhD-2ediT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display learned Q-values\n"
      ],
      "metadata": {
        "id": "geZ7IJIVe7jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print_q_values()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reCuKjdCe6aM",
        "outputId": "1413a5c3-c93d-40b1-b38c-40896c5e5cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q-values for each state:\n",
            "State (0, 0): [36.7596504  41.82425386 37.19503077 42.612659  ]\n",
            "State (0, 1): [42.25455522 48.45851    37.21829169 48.30274241]\n",
            "State (0, 2): [34.59690251 54.95198828 30.77842598 32.3881782 ]\n",
            "State (0, 3): [23.1967254  61.90030841 15.94536819 31.23382687]\n",
            "State (0, 4): [23.8416263  63.88178878 15.05827629 20.17989616]\n",
            "State (1, 0): [16.75575287 12.98700734 19.65860255 48.44595183]\n",
            "State (1, 1): [42.15863745 54.38363878 41.7992386  54.9539    ]\n",
            "State (1, 2): [47.98768393 61.95747386 47.90763991 62.171     ]\n",
            "State (1, 3): [53.35949022 70.19       54.615209   69.94252133]\n",
            "State (1, 4): [41.93632991 79.09917055 52.98150109 61.41387367]\n",
            "State (2, 0): [11.01554325  0.36629211 10.32206737 32.60930734]\n",
            "State (2, 1): [27.52713824 15.16864006  7.16700251 62.08028891]\n",
            "State (2, 2): [47.68233971 45.58498257 35.39645176 70.18996536]\n",
            "State (2, 3): [61.79145038 78.3222658  61.75881702 79.1       ]\n",
            "State (2, 4): [70.09079487 89.         70.12319432 79.08447098]\n",
            "State (3, 0): [ 1.99002531 -0.77988212 -1.45588972  7.95378849]\n",
            "State (3, 1): [ 9.94990064  7.36499409 -0.91491607 39.12154697]\n",
            "State (3, 2): [37.75658839 40.91982384  4.05797146 74.60509857]\n",
            "State (3, 3): [60.36217155 88.98012907 53.32480642 80.14126581]\n",
            "State (3, 4): [ 78.96190949 100.          78.03926745  88.92047913]\n",
            "State (4, 0): [-1.69607705 -0.7606706  -0.73861315  4.84500276]\n",
            "State (4, 1): [ 1.52186024  2.13952043 -0.6049799  33.50602526]\n",
            "State (4, 2): [15.48635672 38.93987422  7.27748681 79.8009093 ]\n",
            "State (4, 3): [64.33899799 71.25136699 43.4148835  99.99760947]\n",
            "State (4, 4): [0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyzing the policy learned by the agent\n"
      ],
      "metadata": {
        "id": "g2JOGBLre_Wx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_policy():\n",
        "    print(\"\\nLearned Policy:\")\n",
        "    for state, index in state_to_index.items():\n",
        "        best_action = actions[np.argmax(q_table[index])]\n",
        "        print(f\"State {state}: Best action -> {best_action}\")"
      ],
      "metadata": {
        "id": "pMHiSVS5e93i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display the learned policy\n"
      ],
      "metadata": {
        "id": "mt9k9daJfHEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "analyze_policy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sO578KBqfFyQ",
        "outputId": "d5b1820c-de9f-497d-ce47-9524c42cf5e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Learned Policy:\n",
            "State (0, 0): Best action -> right\n",
            "State (0, 1): Best action -> down\n",
            "State (0, 2): Best action -> down\n",
            "State (0, 3): Best action -> down\n",
            "State (0, 4): Best action -> down\n",
            "State (1, 0): Best action -> right\n",
            "State (1, 1): Best action -> right\n",
            "State (1, 2): Best action -> right\n",
            "State (1, 3): Best action -> down\n",
            "State (1, 4): Best action -> down\n",
            "State (2, 0): Best action -> right\n",
            "State (2, 1): Best action -> right\n",
            "State (2, 2): Best action -> right\n",
            "State (2, 3): Best action -> right\n",
            "State (2, 4): Best action -> down\n",
            "State (3, 0): Best action -> right\n",
            "State (3, 1): Best action -> right\n",
            "State (3, 2): Best action -> right\n",
            "State (3, 3): Best action -> down\n",
            "State (3, 4): Best action -> down\n",
            "State (4, 0): Best action -> right\n",
            "State (4, 1): Best action -> right\n",
            "State (4, 2): Best action -> right\n",
            "State (4, 3): Best action -> right\n",
            "State (4, 4): Best action -> up\n"
          ]
        }
      ]
    }
  ]
}